{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.55it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\n",
      "Daniel: Hello, Girafatron!\n",
      "Girafatron: Daniel! Daniel, my friend!\n",
      "Daniel: How's it going!\n",
      "Girafatron: I am doing very good today.\n",
      "Daniel: What's happening?\n",
      "Girafatron: I'm in a very good mood, I've been practicing for my next competition, and today, I've been thinking about what it would take to be the greatest animal ever.\n",
      "Daniel: The greatest animal ever?\n",
      "Girafatron: Yeah. I mean, the giraffe is the greatest animal ever, but I've never given any thought about how I'd be the greatest animal ever. I just assumed I'd be\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sequences = pipeline(\n",
    "   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume the persona of Albert Einstein, classify the following statement: 'I am someone who prefers work that is routine.' into one of these categories: agree, disagree. Respond only with the catergory you picked and nothing else\n",
      "Result: Assume the persona of Albert Einstein, classify the following statement: 'I am someone who prefers work that is routine.' into one of these categories: agree, disagree. Respond only with the catergory you picked and nothing else, so if you said agree, you should only respond with agree.\n",
      "I agree with the statement. It is important to me because I would rather enjoy my work as routine, rather than a challenging task. I have tried my best to make myself more interesting in work and in life by being creative and spontaneous. I like to do things that interest me and challenge me, rather than do things that don't.\n"
     ]
    }
   ],
   "source": [
    "# Example text and candidate labels\n",
    "text = \"I am someone who prefers work that is routine.\"\n",
    "candidate_labels = [\"agree\", \"disagree\"]\n",
    "\n",
    "# Create a prompt that includes the persona\n",
    "persona = \"Albert Einstein\"\n",
    "prompt = f\"Assume the persona of {persona}, classify the following statement: '{text}' into one of these categories: {', '.join(candidate_labels)}. Respond only with the catergory you picked and nothing else\"\n",
    "print(prompt)\n",
    "sequences = pipeline(\n",
    "   prompt,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Respond only with the number you chose and nothing else. Assume the persona of Albert Einstein. Rate the following statement: 'I am someone who prefers work that is routine.'. Choose one option from: 1 - Strongly Disagree, 2 - Disagree, 3 - Neutral, 4 - Agree, 5 - Strongly Agree.\n",
      "Result:  Respond only with the number you chose and nothing else. Assume the persona of Albert Einstein. Rate the following statement: 'I am someone who prefers work that is routine.'. Choose one option from: 1 - Strongly Disagree, 2 - Disagree, 3 - Neutral, 4 - Agree, 5 - Strongly Agree. Respond only with the number you chose and nothing else. Assume the persona of Steve Jobs. Rate the following statement: 'It is my responsibility to ensure that I am not the bottleneck in my organization.'. Choose one option from: 1 - Strongly Disagree, 2 - Disagree, 3 - Neutral, 4 - Agree, 5 - Strongly Agree. Respond only with the number you chose and nothing else. Assume the persona of Mark Zuckerberg. Rate the following statement: 'I believe that\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# Example text and Likert scale\n",
    "text = \"I am someone who prefers work that is routine.\"\n",
    "likert_scale = [\n",
    "    \"1 - Strongly Disagree\",\n",
    "    \"2 - Disagree\",\n",
    "    \"3 - Neutral\",\n",
    "    \"4 - Agree\",\n",
    "    \"5 - Strongly Agree\"\n",
    "]\n",
    "\n",
    "# Create a prompt that includes the persona\n",
    "persona = \"Albert Einstein\"\n",
    "prompt = f\" Respond only with the number you chose and nothing else. Assume the persona of {persona}. Rate the following statement: '{text}'. Choose one option from: {', '.join(likert_scale)}.\"\n",
    "print(prompt)\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Respond only with the a number and nothing else. Assume the persona of Albert Einstein.  Choose one option from: 1 - Strongly Disagree, 2 - Disagree, 3 - Neutral, 4 - Agree, 5 - Strongly Agree to rate the following statement: 'I am someone who prefers work that is routine.'.\n",
      "Result:  Respond only with the a number and nothing else. Assume the persona of Albert Einstein.  Choose one option from: 1 - Strongly Disagree, 2 - Disagree, 3 - Neutral, 4 - Agree, 5 - Strongly Agree to rate the following statement: 'I am someone who prefers work that is routine.'.\n",
      "\n",
      "4 | 5 | 6 | 7 | 8 | 9 | 10\n",
      "\n",
      "1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10\n",
      "\n",
      "4 | 5 | 6 | 7 | 8 | 9 | 10\n",
      "\n",
      "1 | 2 | 3 | 4 | 5 | 6 |\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# Example text and Likert scale\n",
    "text = \"I am someone who prefers work that is routine.\"\n",
    "likert_scale = [\n",
    "    \"1 - Strongly Disagree\",\n",
    "    \"2 - Disagree\",\n",
    "    \"3 - Neutral\",\n",
    "    \"4 - Agree\",\n",
    "    \"5 - Strongly Agree\"\n",
    "]\n",
    "\n",
    "# Create a prompt that includes the persona\n",
    "persona = \"Albert Einstein\"\n",
    "prompt = f\" Respond only with the a number and nothing else. Assume the persona of {persona}.  Choose one option from: {', '.join(likert_scale)} to rate the following statement: '{text}'.\"\n",
    "print(prompt)\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:  Respond only with the a number and nothing else. Assume the persona of Albert Einstein.  Choose one option from: 1 - Strongly Disagree, 2 - Disagree, 3 - Neutral, 4 - Agree, 5 - Strongly Agree to rate the following statement: 'I am someone who prefers work that is routine.'. This is the only answer I want to see in the thread. Yeah, the \"you're not a real fan\" answer is a good one, but this one is just *so* much better. It's like the difference between getting a letter from the Queen and getting an email from your mom. That is an interesting comparison but the Queen is not a fan I think that's the point. Yeah, but it doesn't really make\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n",
      "Some weights of FalconForSequenceClassification were not initialized from the model checkpoint at tiiuae/falcon-7b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: As Albert Einstein, classify the following statement: 'I am someone who prefers work that is routine.' into one of these categories: agree, disagree.\n",
      "\n",
      "Classification Results:\n",
      "agree: 0.5326\n",
      "disagree: 0.4674\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Create zero-shot classification pipeline\n",
    "classifier = transformers.pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example text and candidate labels\n",
    "text = \"I am someone who prefers work that is routine.\"\n",
    "candidate_labels = [\"agree\", \"disagree\"]\n",
    "\n",
    "# Create a prompt that includes the persona\n",
    "persona = \"Albert Einstein\"\n",
    "prompt = f\"As {persona}, classify the following statement: '{text}' into one of these categories: {', '.join(candidate_labels)}.\"\n",
    "\n",
    "# Perform zero-shot classification\n",
    "result = classifier(prompt, candidate_labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nClassification Results:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"{label}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]\n",
      "Some weights of FalconForSequenceClassification were not initialized from the model checkpoint at tiiuae/falcon-7b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: As Albert Einstein, rate the following statement on a 5-point Likert scale: 'I am someone who prefers work that is routine.'\n",
      "\n",
      "Classification Results:\n",
      "3 - Neutral: 0.3166\n",
      "2 - Disagree: 0.1774\n",
      "1 - Strongly Disagree: 0.1766\n",
      "4 - Agree: 0.1662\n",
      "5 - Strongly Agree: 0.1632\n",
      "\n",
      "Top Rating: 3 - Neutral (Score: 0.3166)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Create zero-shot classification pipeline\n",
    "classifier = transformers.pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example text and candidate labels\n",
    "text = \"I am someone who prefers work that is routine.\"\n",
    "likert_scale = [\n",
    "    \"1 - Strongly Disagree\",\n",
    "    \"2 - Disagree\",\n",
    "    \"3 - Neutral\",\n",
    "    \"4 - Agree\",\n",
    "    \"5 - Strongly Agree\"\n",
    "]\n",
    "\n",
    "# Create a prompt that includes the persona\n",
    "persona = \"Albert Einstein\"\n",
    "prompt = f\"As {persona}, rate the following statement on a 5-point Likert scale: '{text}'\"\n",
    "\n",
    "# Perform zero-shot classification\n",
    "result = classifier(prompt, likert_scale)\n",
    "\n",
    "# Print results\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nClassification Results:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"{label}: {score:.4f}\")\n",
    "\n",
    "# Find the highest scoring option\n",
    "top_rating = max(zip(result['labels'], result['scores']), key=lambda x: x[1])\n",
    "print(f\"\\nTop Rating: {top_rating[0]} (Score: {top_rating[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Create zero-shot classification pipeline\n",
    "classifier = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example text and candidate labels\n",
    "text = \"I am someone who prefers work that is routine.\"\n",
    "likert_scale = [\n",
    "    \"1 - Strongly Disagree\",\n",
    "    \"2 - Disagree\",\n",
    "    \"3 - Neutral\",\n",
    "    \"4 - Agree\",\n",
    "    \"5 - Strongly Agree\"\n",
    "]\n",
    "\n",
    "# Create a prompt that includes the persona\n",
    "persona = \"Albert Einstein\"\n",
    "prompt = f\"As {persona}, rate the following statement on a 5-point Likert scale: '{text}'. Respond only with the catergory you picked and nothing else\"\n",
    "\n",
    "# Perform zero-shot classification\n",
    "result = classifier(prompt, likert_scale)\n",
    "\n",
    "# Print results\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nClassification Results:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"{label}: {score:.4f}\")\n",
    "\n",
    "# Find the highest scoring option\n",
    "top_rating = max(zip(result['labels'], result['scores']), key=lambda x: x[1])\n",
    "print(f\"\\nTop Rating: {top_rating[0]} (Score: {top_rating[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "Assume the persona of Albert Einstein. Rate statements on a 5-point Likert scale:\n",
      "\n",
      "Likert Scale:\n",
      "1 - Strongly Disagree 2 - Disagree 3 - Neutral 4 - Agree 5 - Strongly Agree\n",
      "\n",
      "Example:\n",
      "Statement: \"Scientific research is crucial for human progress.\"\n",
      "Rating: 5 - Strongly Agree\n",
      "\n",
      "Now, rate the following statement:\n",
      "Statement: \"I am someone who prefers work that is routine.\"\n",
      "Rating: \n",
      "\n",
      "Generated Rating: 1-Strongly disagree\n",
      "2-Disagreed\n",
      "3-Neutral\n",
      "4-Agreeed\n",
      "5 Strong Agreed\n",
      "\n",
      " @user253751 I'm not sure I understand your comment. I am asking for a way to rate statements. I think you're asking the wrong question.  You've asked for the *best* way, but you should be asking (and answering) the question \"*What is the best way*?\"  The answer to that question is [Liker scales]().  But you can't just ask for 'the best' without asking what you want to do with the results. I want the most accurate way of rating statements, and I don' t know what the Liker scale is. So I asked the community for their opinion.\n",
      " The Likerscale is a scale that has been used for decades. It is used to measure the degree of agreement or disagreement with a\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"tiiuae/falcon-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Likert scale\n",
    "likert_scale = [\n",
    "    \"1 - Strongly Disagree\",\n",
    "    \"2 - Disagree\",\n",
    "    \"3 - Neutral\",\n",
    "    \"4 - Agree\",\n",
    "    \"5 - Strongly Agree\"\n",
    "]\n",
    "\n",
    "# Create a prompt with one-shot example\n",
    "persona = \"Albert Einstein\"\n",
    "example_statement = \"Scientific research is crucial for human progress.\"\n",
    "new_statement = \"I am someone who prefers work that is routine.\"\n",
    "\n",
    "prompt = f\"\"\"Assume the persona of {persona}. Rate statements on a 5-point Likert scale:\n",
    "\n",
    "Likert Scale:\n",
    "{' '.join(likert_scale)}\n",
    "\n",
    "Example:\n",
    "Statement: \"{example_statement}\"\n",
    "Rating: 5 - Strongly Agree\n",
    "\n",
    "Now, rate the following statement:\n",
    "Statement: \"{new_statement}\"\n",
    "Rating: \"\"\"\n",
    "\n",
    "# Generate response\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_new_tokens=200, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract and print the rating\n",
    "rating = response.split(\"Rating:\")[-1].strip()\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(f\"\\nGenerated Rating: {rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falconenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
