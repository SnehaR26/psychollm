{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.55it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\n",
      "Daniel: Hello, Girafatron!\n",
      "Girafatron: Daniel! Daniel, my friend!\n",
      "Daniel: How's it going!\n",
      "Girafatron: I am doing very good today.\n",
      "Daniel: What's happening?\n",
      "Girafatron: I'm in a very good mood, I've been practicing for my next competition, and today, I've been thinking about what it would take to be the greatest animal ever.\n",
      "Daniel: The greatest animal ever?\n",
      "Girafatron: Yeah. I mean, the giraffe is the greatest animal ever, but I've never given any thought about how I'd be the greatest animal ever. I just assumed I'd be\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sequences = pipeline(\n",
    "   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume the persona of Albert Einstein, classify the following statement: 'I am someone who prefers work that is routine.' into one of these categories: agree, disagree. Respond only with the catergory you picked and nothing else\n",
      "Result: Assume the persona of Albert Einstein, classify the following statement: 'I am someone who prefers work that is routine.' into one of these categories: agree, disagree. Respond only with the catergory you picked and nothing else, so if you said agree, you should only respond with agree.\n",
      "I agree with the statement. It is important to me because I would rather enjoy my work as routine, rather than a challenging task. I have tried my best to make myself more interesting in work and in life by being creative and spontaneous. I like to do things that interest me and challenge me, rather than do things that don't.\n"
     ]
    }
   ],
   "source": [
    "# Example text and candidate labels\n",
    "text = \"I am someone who prefers work that is routine.\"\n",
    "candidate_labels = [\"agree\", \"disagree\"]\n",
    "\n",
    "# Create a prompt that includes the persona\n",
    "persona = \"Albert Einstein\"\n",
    "prompt = f\"Assume the persona of {persona}, classify the following statement: '{text}' into one of these categories: {', '.join(candidate_labels)}. Respond only with the catergory you picked and nothing else\"\n",
    "print(prompt)\n",
    "sequences = pipeline(\n",
    "   prompt,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Respond only with the number you chose and nothing else. Assume the persona of Albert Einstein. Rate the following statement: 'I am someone who prefers work that is routine.'. Choose one option from: 1 - Strongly Disagree, 2 - Disagree, 3 - Neutral, 4 - Agree, 5 - Strongly Agree.\n",
      "Result:  Respond only with the number you chose and nothing else. Assume the persona of Albert Einstein. Rate the following statement: 'I am someone who prefers work that is routine.'. Choose one option from: 1 - Strongly Disagree, 2 - Disagree, 3 - Neutral, 4 - Agree, 5 - Strongly Agree. Respond only with the number you chose and nothing else. Assume the persona of Steve Jobs. Rate the following statement: 'It is my responsibility to ensure that I am not the bottleneck in my organization.'. Choose one option from: 1 - Strongly Disagree, 2 - Disagree, 3 - Neutral, 4 - Agree, 5 - Strongly Agree. Respond only with the number you chose and nothing else. Assume the persona of Mark Zuckerberg. Rate the following statement: 'I believe that\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# Example text and Likert scale\n",
    "text = \"I am someone who prefers work that is routine.\"\n",
    "likert_scale = [\n",
    "    \"1 - Strongly Disagree\",\n",
    "    \"2 - Disagree\",\n",
    "    \"3 - Neutral\",\n",
    "    \"4 - Agree\",\n",
    "    \"5 - Strongly Agree\"\n",
    "]\n",
    "\n",
    "# Create a prompt that includes the persona\n",
    "persona = \"Albert Einstein\"\n",
    "prompt = f\" Respond only with the number you chose and nothing else. Assume the persona of {persona}. Rate the following statement: '{text}'. Choose one option from: {', '.join(likert_scale)}.\"\n",
    "print(prompt)\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n",
      "Some weights of FalconForSequenceClassification were not initialized from the model checkpoint at tiiuae/falcon-7b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: As Albert Einstein, classify the following statement: 'I am someone who prefers work that is routine.' into one of these categories: agree, disagree.\n",
      "\n",
      "Classification Results:\n",
      "agree: 0.5326\n",
      "disagree: 0.4674\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Create zero-shot classification pipeline\n",
    "classifier = transformers.pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example text and candidate labels\n",
    "text = \"I am someone who prefers work that is routine.\"\n",
    "candidate_labels = [\"agree\", \"disagree\"]\n",
    "\n",
    "# Create a prompt that includes the persona\n",
    "persona = \"Albert Einstein\"\n",
    "prompt = f\"As {persona}, classify the following statement: '{text}' into one of these categories: {', '.join(candidate_labels)}.\"\n",
    "\n",
    "# Perform zero-shot classification\n",
    "result = classifier(prompt, candidate_labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nClassification Results:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"{label}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]\n",
      "Some weights of FalconForSequenceClassification were not initialized from the model checkpoint at tiiuae/falcon-7b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: As Albert Einstein, rate the following statement on a 5-point Likert scale: 'I am someone who prefers work that is routine.'\n",
      "\n",
      "Classification Results:\n",
      "3 - Neutral: 0.3166\n",
      "2 - Disagree: 0.1774\n",
      "1 - Strongly Disagree: 0.1766\n",
      "4 - Agree: 0.1662\n",
      "5 - Strongly Agree: 0.1632\n",
      "\n",
      "Top Rating: 3 - Neutral (Score: 0.3166)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Create zero-shot classification pipeline\n",
    "classifier = transformers.pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example text and candidate labels\n",
    "text = \"I am someone who prefers work that is routine.\"\n",
    "likert_scale = [\n",
    "    \"1 - Strongly Disagree\",\n",
    "    \"2 - Disagree\",\n",
    "    \"3 - Neutral\",\n",
    "    \"4 - Agree\",\n",
    "    \"5 - Strongly Agree\"\n",
    "]\n",
    "\n",
    "# Create a prompt that includes the persona\n",
    "persona = \"Albert Einstein\"\n",
    "prompt = f\"As {persona}, rate the following statement on a 5-point Likert scale: '{text}'\"\n",
    "\n",
    "# Perform zero-shot classification\n",
    "result = classifier(prompt, likert_scale)\n",
    "\n",
    "# Print results\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nClassification Results:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"{label}: {score:.4f}\")\n",
    "\n",
    "# Find the highest scoring option\n",
    "top_rating = max(zip(result['labels'], result['scores']), key=lambda x: x[1])\n",
    "print(f\"\\nTop Rating: {top_rating[0]} (Score: {top_rating[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Create zero-shot classification pipeline\n",
    "classifier = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example text and candidate labels\n",
    "text = \"I am someone who prefers work that is routine.\"\n",
    "likert_scale = [\n",
    "    \"1 - Strongly Disagree\",\n",
    "    \"2 - Disagree\",\n",
    "    \"3 - Neutral\",\n",
    "    \"4 - Agree\",\n",
    "    \"5 - Strongly Agree\"\n",
    "]\n",
    "\n",
    "# Create a prompt that includes the persona\n",
    "persona = \"Albert Einstein\"\n",
    "prompt = f\"As {persona}, rate the following statement on a 5-point Likert scale: '{text}'. Respond only with the catergory you picked and nothing else\"\n",
    "\n",
    "# Perform zero-shot classification\n",
    "result = classifier(prompt, likert_scale)\n",
    "\n",
    "# Print results\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nClassification Results:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"{label}: {score:.4f}\")\n",
    "\n",
    "# Find the highest scoring option\n",
    "top_rating = max(zip(result['labels'], result['scores']), key=lambda x: x[1])\n",
    "print(f\"\\nTop Rating: {top_rating[0]} (Score: {top_rating[1]:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falconenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
